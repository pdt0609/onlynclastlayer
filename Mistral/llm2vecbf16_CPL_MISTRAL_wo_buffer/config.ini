[GPT]
gpt_temp = 0.0
key = your_openAI_api_keyproj-iOvFbA-CLbAjJT3bafPxp4A2L44GcZjvR1qRZP70yukFWjP6xqU_XGN-V2BNPwKID-WQq9zZZgT3BlbkFJhTYj72AJKy8U-sjz3MSu9n1Gr_yS0dsejyOvhEPWYKO6hBOPwm30DleOFFzP-BwEv5p5lI54UA
[task]
seed = 100
device = cuda
task_name = FewRel
;FewRel, Tacred

[continual]
num_k = 5
; num_k = 5-shot, 10-shot
pattern = hardprompt
; pattern = marker,hardprompt,softprompt,cls,hybridprompt
total_round = 6
task_length = 8
memory_size = 1

[datageneration]
gen = 1
;gen = data generation open or not
num_gen = 2
num_gen_augment = 3

[training]
batch_size = 4
epoch = 10
epoch_mem = 10
lr = 0.00001
num_workers = 2

[contrastive]
margin = 0.3
sample_k = 500
contrastive_temp = 0.1

[softprompt]
tune = all
; tune = prompt, all
prompt_init = 0
; prompt_init = 0: random, 
; prompt_init = 1: is, 
; prompt_init = 2: ! @ # [e1] he is as [MASK] * & % [e2] just do it 
prompt_len = 3
prompt_num = 4

[Encoder]
model = bert
; model = roberta, bert 
bert_path = google-bert/bert-base-uncased
llm_path = dunzhang/stella_en_1.5B_v5
model_dir = ./model_stella 
; bert_path = BAAI/bge-base-en-v1.5

roberta_path = ./roberta-base
max_length = 256
encoder_output_size = 4096