[GPT]
gpt_temp = 0.0
key = your_openAI_api_keyproj-x71uAfHhhMxvtkPXOlX6Ubvtb5vP4_9o4Zet1X_va2a65m2AtKlcShrYR7-EUEdVL22a5TQm12T3BlbkFJY_0a61wQE6O_B7Ll_IAUbIRb1irTa793KFgVGc_P2SU_vp4BP8ahG7CV0KlXJj0P99l1nWw0sA
[task]
seed = 100
device = cuda
task_name = FewRel
;FewRel, Tacred

[continual]
num_k = 5
; num_k = 5-shot, 10-shot
pattern = hardprompt
; pattern = marker,hardprompt,softprompt,cls,hybridprompt
total_round = 6
task_length = 8
memory_size = 1

[datageneration]
gen = 1
;gen = data generation open or not
num_gen = 2
num_gen_augment = 3

[training]
batch_size = 4
epoch = 10
epoch_mem = 10
lr = 0.00001
num_workers = 2

[contrastive]
margin = 0.3
sample_k = 500
contrastive_temp = 0.1

[softprompt]
tune = all
; tune = prompt, all
prompt_init = 0
; prompt_init = 0: random, 
; prompt_init = 1: is, 
; prompt_init = 2: ! @ # [e1] he is as [MASK] * & % [e2] just do it 
prompt_len = 3
prompt_num = 4

[Encoder]
model = bert
; model = roberta, bert 
bert_path = google-bert/bert-base-uncased
llm_path = dunzhang/stella_en_1.5B_v5
; bert_path = BAAI/bge-base-en-v1.5

roberta_path = ./roberta-base
max_length = 256
encoder_output_size = 4096